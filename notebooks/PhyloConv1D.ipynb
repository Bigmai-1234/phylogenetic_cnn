{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is required in order to be able to do relative imports like phcnn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change PYTHONPATH to allow for relative import\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Lambda, MaxPooling1D, Flatten,\n",
    "                          Dropout, Dense, Input)\n",
    "from keras.models import Model\n",
    "from keras.backend import floatx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "D:\\devtools\\anaconda\\envs\\xbiome\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from phcnn.layers import PhyloConv1D, euclidean_distances\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Parameters from convolutional layer. nb_neighbors is the number of neighbors to be convoluted together, nb_filters is the number of convolutional filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_neighbors = 4\n",
    "nb_filters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of data\n",
    "\n",
    "We need to expand Xs to be of the shape (filters, nb_samples, nb_features), so we apply a np.expand_dims to signal that we have only one filter. Futhermore we need to have y in a categorical form so we apply to_categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xs = pd.read_csv('../datasets/ibd_dataset/HS_CDf/Sokol_16S_taxa_HS_CDf_commsamp_training.txt',\n",
    "                 sep='\\t', header=0, index_col=0)\n",
    "nb_features = Xs.shape[1]\n",
    "Xs = np.expand_dims(Xs, axis=-1)\n",
    "y = np.loadtxt('../datasets/ibd_dataset/HS_CDf/Sokol_16S_taxa_HS_CDf_commsamp_training_lab.txt', dtype=np.int)\n",
    "Y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore we need to import the MDS coordinates for our features. \n",
    "\n",
    "Pre-computed coordinates have been made available in the repository in the `datasets/coordinates`, for each of the diseases included in the **IBD** dataset.\n",
    "\n",
    "Keras uses fast symbolic mathematical libraries as a backend, such as TensorFlow and Theano.\n",
    "\n",
    "A downside of using these libraries is that the shape and size of your data must be defined once up front and held constant regardless of whether you are training your network or making predictions.\n",
    "\n",
    "To fit the coordinate matrix as a valid Keras Tensor, we need to pre-process the numpy array to properly match dimensions. In more details, the first dimension of a tensor object must correspond to `n_samples`, that is the number of samples (_per batch_).\n",
    "\n",
    "Thus we need to replicate **feature** coordinates for all the samples so that each sample  provide  padding to loaded numpy array for the coordinates. This is because, i We choosed to do it in the most straigthforward way possibile, we simply duplicate the coordinate matrix for every sample. We will drop such padding after the matrix is loaded in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.read_csv('../datasets/coordinates/coordinates_cdf.txt',\n",
    "                sep='\\t', header=0, index_col=0)\n",
    "nb_coordinates = C.shape[0]\n",
    "Coords = np.empty((Xs.shape[0],) + C.shape, dtype=np.float64)\n",
    "for i in range(Xs.shape[0]):\n",
    "    Coords[i] = C\n",
    "# add last dimension, i.e. channel, necessary for the Convolution\n",
    "# operator to work.\n",
    "Coords = np.expand_dims(Coords, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((306, 259), (69, 306, 259, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape,Coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Input(shape=(nb_features, 1), name=\"data\", dtype=tf.float32)\n",
    "coordinates = Input(shape=(nb_coordinates, nb_features, 1),\n",
    "                            name=\"coordinates\", dtype=tf.float32)\n",
    "\n",
    "conv_layer = data\n",
    "# We remove the padding that we added to work around keras limitations\n",
    "conv_crd = Lambda(lambda c: c[0], output_shape=lambda s: (s[1:]))(coordinates)\n",
    "\n",
    "distances = euclidean_distances(conv_crd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'data:0' shape=(?, 259, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "conv_layer, conv_crd = PhyloConv1D(distances, nb_neighbors, nb_filters, activation='relu')([conv_layer, conv_crd])\n",
    "\n",
    "\n",
    "max = MaxPooling1D(pool_size=2, padding=\"valid\")(conv_layer)\n",
    "flatt = Flatten()(max)\n",
    "drop = Dropout(0.25)(Dense(units=64, activation='relu')(flatt))\n",
    "output = Dense(units=2, kernel_initializer=\"he_normal\",\n",
    "               activation=\"softmax\", name='output')(drop)\n",
    "\n",
    "model = Model(inputs=[data, coordinates], outputs=output)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.6977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c8830ff98>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[Xs, Coords], y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xbiome",
   "language": "python",
   "name": "xbiome"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
